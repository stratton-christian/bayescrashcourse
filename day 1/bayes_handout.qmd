---
title: "Bayesian inference"
format:
  pdf:
    fontsize: 12pt
    fig-align: center
    mathspec: true
    geometry:
      - inner=1.5cm
      - outer=1.5cm
      - top=2.5cm
      - bottom=2.5cm
    header-includes: |
      \addtokomafont{disposition}{\rmfamily}
      \usepackage{fancyhdr, lastpage, framed, caption, xcolor, setspace, multirow, tabularray}
      \captionsetup[figure]{labelformat=empty}
      \pagestyle{fancyplain}
      \fancyhf{}
      \lhead{\fancyplain{}{Bayes Crash Course}}
      \rhead{\fancyplain{}{Stratton - Day 1}}
      \fancyfoot[RO, LE] {page \thepage\ of \pageref{LastPage}}
      \thispagestyle{plain}
      \usepackage{pdflscape}
      \fancypagestyle{mylandscape}{
        \fancyhf{} %Clears the header/footer
        \renewcommand{\headrulewidth}{0pt}% No header rule
        \renewcommand{\footrulewidth}{0pt}% No footer rule
      }
      \DeclareMathOperator*{\argmax}{arg\,max}
      \usepackage{stackengine,trimclip,scalerel}
      \savestack\eye{\rotatebox{90}{$^\circ\mkern-6mu\raisebox{1pt}{)}$}}
      \savestack\nose{\raisebox{3pt}{\scalebox{1}[-1]{\clipbox{0pt 1pt 0pt 0pt}{?}}}}
      \savestack\mouth{\rotatebox{90}{(}}
      \newcommand\Lenny{(\scalerel{\stackanchor[2pt]{\eye \nose \eye}{\mouth}}{)}}
editor: source
---

```{r, include = F}
rm(list = ls())
library(tidyverse)
library(gtools)
```

\vspace{-1in}

## Introduction

The purpose of this crash course is to develop a working knowledge of Bayesian statistics. We will focus on 1) the philosophy of Bayesian statistics, 2) how to use probabilistic programming languages to estimate Bayesian models, 3) how to ensure MCMC estimation of Bayesian models has converged, 4) how to interpret results of model fitting, and 5) how to assess and compare fitted models. This is by no means comprehensive - look forward to Bayes with Becky in the Fall!

\newpage

\begin{landscape}
\thispagestyle{mylandscape}

\definecolor{lg}{RGB}{245,246,246}
\begin{tblr}{
colspec=lccccc, hline{2},
row{2,3,6,7,11,12,15,16,19,20,23,24} = {bg=lg},
cell{2}{2}={r=2}{c}, cell{2}{4}={r=2}{c}, cell{2}{5}={r=2}{c}, cell{2}{6}={r=2}{c},
cell{4}{2}={r=2}{c}, cell{4}{4}={r=2}{c}, cell{4}{5}={r=2}{c}, cell{4}{6}={r=2}{c},
cell{6}{2}={r=2}{c}, cell{6}{4}={r=2}{c}, cell{6}{5}={r=2}{c}, cell{6}{6}={r=2}{c},
cell{8}{1}={r=3}{l}, cell{8}{4}={r=3}{c}, cell{8}{5}={r=3}{c}, cell{8}{6}={r=3}{c},
cell{11}{2}={r=2}{c}, cell{11}{4}={r=2}{c}, cell{11}{5}={r=2}{c}, cell{11}{6}={r=2}{c},
cell{13}{4}={r=2}{c}, cell{13}{5}={r=2}{c}, cell{13}{6}={r=2}{c},
cell{15}{2}={r=2}{c}, cell{15}{4}={r=2}{c}, cell{15}{5}={r=2}{c}, cell{15}{6}={r=2}{c},
cell{17}{2}={r=2}{c}, cell{17}{4}={r=2}{c}, cell{17}{5}={r=2}{c}, cell{17}{6}={r=2}{c},
cell{19}{2}={r=2}{c}, cell{19}{4}={r=2}{c}, cell{19}{5}={r=2}{c}, cell{19}{6}={r=2}{c},
cell{21}{2}={r=2}{c}, cell{21}{4}={r=2}{c}, cell{21}{5}={r=2}{c}, cell{21}{6}={r=2}{c},
cell{23}{2}={r=2}{c}, cell{23}{4}={r=2}{c}, cell{23}{5}={r=2}{c}, cell{23}{6}={r=2}{c},
}

\textbf{Distribution} & \textbf{Parameters} & \textbf{Probability function} & \textbf{Mean} & \textbf{Variance} & \textbf{MGF} \\

Bernoulli & $p \in [0, 1]$ & $f(x) = p^x(1-p)^{1-x}$; & $p$ & $p(1-p)$ & $pe^t + (1 - p)$ \\
$\text{Bern}(p)$ & 3-2 & $x \in \{0, 1\}$ & 3-4 & 3-5 & 3-6 \\

Binomial & $p \in [0, 1]$ & $f(x) = \binom{n}{x}p^x(1-p)^{n-x}$; & $np$ & $np(1-p)$ & $\left[pe^t + (1 - p) \right]^n$ \\
$\text{Bin}(p)$ & 5-2 & $x \in \{0, 1, ..., n\}$ & 5-4 & 5-5 & 5-6 \\

Geometric & $p \in [0, 1]$ & $f(x) = p(1 - p)^{x-1}$; & $\frac{1}{p}$ & $\frac{1 - p}{p^2}$ & $\frac{pe^t}{1 - (1 - p)e^t}$\\
$\text{Geom}(p)$ & 7-2 & $x \in \{1, 2, \dots \}$ & 7-4 & 7-5 & 7-6 \\

Hypergeometric & $ N \in \{0, 1, \dots\}$ & $f(x) = \frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}}$; & $\frac{nr}{N}$ & $n \left( \frac{n}{r} \right) \left( \frac{N-r}{N} \right) \left( \frac{N-n}{N-1} \right)$ & Don't bother \\
9-1 & $r \in \{0, 1, \dots, N\}$ & $x \in \{0, 1, \dots, n\}$ if $n\leq r$, & 9-4 & 9-5 & 9-6 \\
10-1 & $n \in \{0, 1, \dots, N\}$ & $x \in \{0, 1, \dots, r\}$ if $n > r$ & 10-4 & 10-5 & 10-6 \\

Poisson & $\lambda > 0$ & $f(x) = \frac{e^{-\lambda}\lambda^x}{x!}$; & $\lambda$ & $\lambda$ & $\exp\left[\lambda(e^t - 1) \right]$ \\
$\text{Pois}(\lambda)$ & & $x \in \{0, 1, \dots\}$ & & & \\

Negative binomial & $r \in \{0, 1, \dots\}$ & $f(x) = \binom{x + r + 1}{x}p^r(1-p)^x$; & $\frac{r(1-p)}{p}$ & $\frac{r(1-p)}{p^2}$ & $\left(\frac{p}{1 - (1-p)e^t} \right)^r$ \\
$\text{NegBin}(r, p)$ & $p \in [0, 1]$ & $x \in \{0, 1, \dots\}$ & & & \\

Beta & $a, b > 0$ & $f(x) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1}$; & $\frac{a}{a+b}$ & $\frac{ab}{(a + b)^2(a + b + 1)}$ & DNE \\
$\text{beta(a, b)}$ & & $x \in (0, 1)$ & & & \\

Chi-square & $\nu \in \{1, 2, \dots\}$ & $f(x) = \frac{1}{2^{\nu/2}\Gamma(\frac{\nu}{2})}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}$; & $\nu$ & $2\nu$ & $(1-2t)^{-v/2}$ \\
$\chi^2_\nu$ & & $x > 0$ & & & \\

Exponential & $\lambda > 0$ & $f(x) = \lambda e^{-\lambda x}$; & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ & $\frac{\lambda}{\lambda - t}$ \\
$\text{Exp}(\lambda)$ & & $x > 0$ & & & \\

Gamma & $\alpha, \beta > 0$ & $f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1} e^{-\beta x}$; & $\frac{\alpha}{\beta}$ & $\frac{\alpha}{\beta^2}$ & $\left(1 - \frac{t}{\beta}\right)^{-\alpha}$ \\
$\text{Ga}(\alpha, \beta)$ & & $x > 0$ & & & \\

Normal & $\mu \in (-\infty, \infty)$ & $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2 \right\}$;  & $\mu$ & $\sigma^2$ & $\exp\left\{\mu t + \frac{t^2\sigma^2}{2}\right\}$ \\
$N(\mu, \sigma^2)$ & & $x \in (-\infty, \infty)$ & & & \\

Uniform & $\{\theta_1, \theta_2: \theta_1 < \theta_2\}$ & $f(x) = \frac{1}{\theta_2 - \theta_1}$;  & $\frac{1}{2}(\theta_1+\theta_2)$ & $\frac{1}{12}(\theta_2-\theta_1)^2$ & $\frac{e^{t\theta_2}-e^{t\theta_1}}{t(\theta_2 - \theta_1)}$ \\
$\text{Unif}(\theta_1, \theta_2)$ & & $\theta_1 \leq x \leq \theta_2$ & & & \\
\end{tblr}
\end{landscape}

\newpage

## The Bayesian statistical paradigm

:::{.callout-note icon=false title="Bayesian statistics"}
1. A model for the data generating mechanism is specified in terms of probability distributions with unknown parameters. The model should be specified in a way that the questions you have of the data may be asked of the model parameters.

2. Your prior belief about the unknown parameters is expressed in a probabilistic way via the **prior distribution**, $p(\theta)$.

3. Data are collected and are modeled via a probability distribution, $p(\boldsymbol{y} | \theta)$ (i.e. the likelihood or sampling model)

4. Your updated belief in $\theta$ is expressed via the **posterior distribution**, according to Bayes' rule:
$$
p(\theta | \boldsymbol{y}) = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{p(\boldsymbol{y})} = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{\int_\theta p(\boldsymbol{y}|\theta) p(\theta) d\theta}
$$

5. All subsequent inference is based on the posterior distribution. 
:::


```{r, echo = F, fig.dim = c(8, 5), fig.align='center'}
p <- seq(0, 1, length.out = 1000)
a <- b <- 3
prior <- dbeta(p, a, b)
likelihood <- dbinom(7, 10, p)
posterior <- dbeta(p, 7 + a, 10 - 7 + b)
tibble(
  p = p, 
  prior = prior,
  likelihood = likelihood,
  posterior = posterior
) %>%
  pivot_longer(prior:posterior, names_to = "Distr") %>%
  ggplot() + 
  geom_area(
    aes(x = p, y = value, fill = Distr),
    position = "identity",
    alpha = .3
  ) +
  theme_bw() +
  labs(
    y = ""
  )
```

\newpage

## Example research questions

Suppose you are interested in modeling the count of invasive weeds obtained from various locations throughout a National Park as a function of precipitation. 


\vfill

Suppose 10 students each swab their mouths five times and you are interested in modeling the presence of a particular bacteria in each students' mouth. 


\vfill


\newpage

## Bayesian estimation

Once you have specified your prior distribution ($p(\theta)$) and likelihood ($p(\boldsymbol{y}|\theta)$), the posterior may be obtained according to Bayes' rule:
$$
p(\theta | \boldsymbol{y}) = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{p(\boldsymbol{y})} = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{\int_\theta p(\boldsymbol{y}|\theta) p(\theta) d\theta}
$$
For most problems, this mathematical expression is not tractable because the integral in the denominator has no elementary anti-derivative. There exist combinations of priors and likelihoods for which there are analytic solutions for the posterior distribution, called **conjugate priors** - we will not focus on those this week. Instead, we will consider a flexible approach to finding the posterior distribution of a parameter, called **Markov Chain Monte Carlo** (MCMC). 

:::{.callout-note icon=false title="Markov-Chain Monte Carlo"}
When we cannot find a closed form expression for the posterior distribution, we instead try to *sample from the unknown density function* by generating a Markov Chain whose stationary distribution is the joint posterior distribution of the unknown parameters. Thus, samples drawn from this Markov chain may be used to represent the unknown posterior density.

\vspace{.125in}

There exist many strategies for developing this Markov chain, including **Gibbs sampling**, which I expect you will see in the Fall. Today, we will focus on a very general algorithm for generating this Markov chain, called the **Metropolis-Hastings** algorithm, which proceeds as follows:

1. Initialize the vector of unknown parameters $\theta^{(s)}$.

2. Choose a "transition kernel" or "proposal distribution" $Q(\theta^* | \theta^{(s)})$, a way of proposing a new set of parameter values $\theta^*$ given $\theta^{(s)}$. This kernel should be symmetric, and most often is chosen to be a random walk. For instance, 
$$
\theta^* | \theta^{(s)} \sim \mathcal{N}(\theta^{(s)}, \tau_0^2 \mathcal{I})
$$
3. For $s$ in `2:iter`:

   - sample $\theta^*$ from $Q(\theta^* | \theta^{(s)})$
  
   - compute the acceptance probability
$$
q = \text{min}\left(1, \frac{p(\theta^* | \boldsymbol{y}) Q(\theta^{(s)} | \theta^*)}{p(\theta^{(s)} | \boldsymbol{y})Q(\theta^* | \theta^{(s)})} \right)
$$

   - with probability $q$, set $\theta^{(s+1)}=\theta^*$. Otherwise, set $\theta^{(s+1)}=\theta^{(s)}$.

:::

\newpage

## Probabilistic programming languages

Implementing Metropolis-Hastings samplers by hand in R can be very fun, but also time consuming. Fortunately, there exist multiple flexible tools to implement Metropolis-Hastings, and usually with slightly more efficient proposal distributions, called **probabilistic programming languages** (PPLs). There exist many, but we will focus on `stan` and `nimble`. 

```{r, include = F}
library(nimble)
library(rstan)

# stan options
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()-1)
```

Suppose we observe the following data:

```{r, message = F, fig.align='center'}
y <- rpois(20, lambda = 5)
y
tibble(y = y) %>% ggplot() + geom_dotplot(aes(x = y)) + theme_bw()
```

Let's use nimble and stan to estimate the $\lambda$ parameter of the Poisson distribution. See the [Nimble documentation](https://r-nimble.org/html_manual/cha-writing-models.html#sec:supp-feat-bugs) and [Stan documentation](https://mc-stan.org/docs/2_36/stan-users-guide/) for help! In each of the following subsections, we 

- Use the PPL to sample from the posterior distribution

- Assess whether the algorithm converged using trace plots and the **Gelman-Rubin statistic** ($\hat{R}$)

- Obtain posterior summaries and **highest density intervals**

- Assess how well the model fit by examining the **posterior predictive distribution**

\newpage

### Nimble

Step 1: 

**Likelihood/sampling model**

$$
\begin{split}
y_i \sim \text{Poisson}(\lambda) 
\end{split}
$$

**Priors** (pick one of these)

$$
\begin{split}
\lambda &\sim \text{N}_{(0, \infty)}(0, 100) \\
\log \lambda &\sim N(0, 100) \\
\lambda &\sim \text{Uniform}(0, 10000) \\
\lambda &\sim \text{Gamma}(.001, .001)
\end{split}
$$

```{r}
# inspect priors
hist(abs(rnorm(10000, 0, 100))) # first prior
hist(exp(rnorm(10000, 0, 100))) # second prior
hist(runif(10000, 0, 100000)) # third prior
hist(rgamma(10000, .001, .001), xlim = c(0, 100), breaks = 50) # four prior
```

```{r, eval = F}
# nimble code for model
code <- nimbleCode({
  # priors 
  lambda ~ T(dnorm(0, sd = 100), 0, Inf) # half-normal prior
  
  # likelihood as a loop
  for(i in 1:n){
    y[i] ~ dpois(lambda)
  }
})

# build model
rmodel <- nimbleModel(
  code = code, # model code
  data = list(y = y), # list of data (the random stuff)
  constants = list(n = length(y)), 
  inits = list(lambda = 500)
) # model in R
cmodel <- compileNimble(rmodel) # model in C++

# build MCMC
mcmc_conf <- configureMCMC(cmodel)
rmcmc <- buildMCMC(mcmc_conf) # MCMC in R
cmcmc <- compileNimble(rmcmc, project = cmodel) # MCMC in C++

# run the MCMC
samples <- runMCMC(
  cmcmc,
  niter = 10000, # number of posterior samples
  nburnin = 5000, # how many posterior samples to discard
  thin = 1, # interval of samples to keep 
  nchains = 4, # number of independent Markov chains to run 
  samplesAsCodaMCMC = TRUE # nice formatting
)

```

```{r, eval = F}
# lots of options here - can use ggmcmc, bayesplot, coda, posterior, or by hand!
library(ggmcmc)
ggmcmc_samples <- ggs(samples)

# built in summary function
summary(samples)

# can also manipulate the samples manually!
do.call("rbind", samples) %>% colMeans()

# traceplots and convergence
## do they look like fuzzy catepillars?
### using ggmcmc
ggs_traceplot(ggmcmc_samples) + theme_bw() 

### by hand
as_tibble(do.call("rbind", samples)) %>% 
  pivot_longer(everything(), names_to = "param", values_to = "trace") %>%
  mutate(
    iter = rep(1:nrow(samples[[1]]), length(samples)),
    chain = rep(1:length(samples), each = nrow(samples[[1]])) %>% factor(), 
  ) %>%
  ggplot() + 
  geom_line(aes(x = iter, y = trace, col = chain)) +
  facet_wrap(~ param) +
  theme_bw()

# bayestestr? new to me!
library(bayestestR)
describe_posterior(samples)
hdi(samples) # this is nice

# posterior - new to me, but I like it
library(posterior)
samples_posterior <- as_draws_matrix(samples)
posterior::summarise_draws(
  samples_posterior
)
```

```{r, eval = F}
library(bayesplot)

# posterior predictive checks 
samples_mat <- do.call("rbind", samples)
ppc <- matrix(NA, nrow(samples_mat), length(y)) 
for(i in 1:nrow(ppc)){
  ppc[i,] <- rpois(ncol(ppc), samples_mat[i,])
}

# check out a few
bayesplot::pp_check(
  y, ppc[sample(1:nrow(ppc), 100),], ppc_dens_overlay
)

# can do it by hand to make it look a little nicer
ppc_df <- t(rbind(y, ppc[sample(1:nrow(ppc), 100), ])) %>%
  as_tibble() %>%
  pivot_longer(everything(), names_to = "var", values_to = "val") %>%
  mutate(obs = ifelse(var == "y", "y", "yrep")) 
ggplot() + 
  geom_density(
    data = ppc_df %>% filter(obs != "y"),
    aes(x = val, group = var), col = "lightblue", alpha = .5
  ) +  
  geom_density(
    data = ppc_df %>% filter(obs == "y"),
    aes(x = val), size = 1.1
  ) + 
  theme_bw() +
  labs(x = "")
```

\newpage

### Stan

```{r, eval = F}
# lets use cmdstanr
rm(list = ls()[-which(ls() == "y")])
library(cmdstanr)
# doesn't need to be this nicely formatted
stan_mod <- cmdstan_model(
  stan_file = write_stan_file(
    "
    data {
      int<lower=0> N;
      array[N] int<lower=0> y;
    }
    
    parameters {
      real<lower=0> lambda;
    }
    
    model {
      // likelihood
      y ~ poisson(lambda);
      
      // priors
      lambda ~ normal(0, 100);
    }
    "
  )
)
dat <- list(
  N = length(y),
  y = y
)
fit <- stan_mod$sample(
  data = dat, 
  chains = 4, 
  parallel_chains = 4
)
# see ?CmdStanMCMC
```

```{r, eval = F}
# built-in summary is nice
library(posterior)
library(coda)
library(cmdstanr)
library(ggmcmc)
samples_array <- fit$draws() # new array format
samples <- mcmc.list(
  lapply(1:posterior::nchains(samples_array), function(i) {
    mcmc(as.matrix(samples_array[,i,,drop = T]))
  })
) # old coda format
fit$summary()

# convergence
ggs_traceplot(ggs(samples)) + theme_bw() 
as_tibble(do.call("rbind", samples)) %>% 
  mutate(
    iter = rep(1:nrow(samples[[1]]), length(samples)),
    chain = rep(1:length(samples), each = nrow(samples[[1]])) %>% factor(), 
  ) %>%
  dplyr::select(iter, chain, everything()) %>%
  pivot_longer(-c(1:2), names_to = "param", values_to = "trace") %>%
  arrange(param, chain, iter) %>%
  ggplot() + 
  geom_line(aes(x = iter, y = trace, col = chain)) +
  facet_wrap(~ param, scales = "free_y", nrow = 2) +
  theme_bw()

# hdi 
bayestestR::hdi(samples_array)

# kind of fun 
library(shinystan)
launch_shinystan(fit)
```

```{r, eval = F}
library(bayesplot)

# posterior predictive checks 
samples_mat <- do.call("rbind", samples)
ppc <- matrix(NA, nrow(samples_mat), length(y)) 
for(i in 1:nrow(ppc)){
  ppc[i,] <- rpois(ncol(ppc), samples_mat[i,2])
}

# check out a few
bayesplot::pp_check(
  y, ppc[1:50,], ppc_dens_overlay
)

# can do it by hand to make it look a little nicer
ppc_df <- t(rbind(y, ppc[sample(1:nrow(ppc), 50), ])) %>%
  as_tibble() %>%
  pivot_longer(everything(), names_to = "var", values_to = "val") %>%
  mutate(obs = ifelse(var == "y", "y", "yrep")) 
ggplot() + 
  geom_density(
    data = ppc_df %>% filter(obs != "y"),
    aes(x = val, group = var), col = "lightblue", alpha = .5
  ) +  
  geom_density(
    data = ppc_df %>% filter(obs == "y"),
    aes(x = val), linewidth = 1.1
  ) + 
  theme_bw() +
  labs(x = "")
```


### Summarizing

Upon estimating a model using MCMC, you should:

- Assess convergence using $\hat{R}$ and trace-plots

- Summarize the posterior to obtain posterior means/medians (point estimates) and highest density intervals/credibility intervals (uncertainty)

- Create posterior predictive checks to assess how well your model is fitting the observed data

\newpage
