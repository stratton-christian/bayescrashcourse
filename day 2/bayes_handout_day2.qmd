---
title: "Bayesian inference day 2"
format:
  pdf:
    fontsize: 12pt
    fig-align: center
    mathspec: true
    geometry:
      - inner=1.5cm
      - outer=1.5cm
      - top=2.5cm
      - bottom=2.5cm
    header-includes: |
      \addtokomafont{disposition}{\rmfamily}
      \usepackage{fancyhdr, lastpage, framed, caption, xcolor, setspace, multirow, tabularray}
      \captionsetup[figure]{labelformat=empty}
      \pagestyle{fancyplain}
      \fancyhf{}
      \lhead{\fancyplain{}{Bayes Crash Course}}
      \rhead{\fancyplain{}{Stratton - Day 1}}
      \fancyfoot[RO, LE] {page \thepage\ of \pageref{LastPage}}
      \thispagestyle{plain}
      \usepackage{pdflscape}
      \fancypagestyle{mylandscape}{
        \fancyhf{} %Clears the header/footer
        \renewcommand{\headrulewidth}{0pt}% No header rule
        \renewcommand{\footrulewidth}{0pt}% No footer rule
      }
      \DeclareMathOperator*{\argmax}{arg\,max}
      \usepackage{stackengine,trimclip,scalerel}
      \savestack\eye{\rotatebox{90}{$^\circ\mkern-6mu\raisebox{1pt}{)}$}}
      \savestack\nose{\raisebox{3pt}{\scalebox{1}[-1]{\clipbox{0pt 1pt 0pt 0pt}{?}}}}
      \savestack\mouth{\rotatebox{90}{(}}
      \newcommand\Lenny{(\scalerel{\stackanchor[2pt]{\eye \nose \eye}{\mouth}}{)}}
editor: source
---

```{r, include = F}
rm(list = ls())
library(tidyverse)
library(gtools)
```

\vspace{-1in}

## Introduction

Today, we start working with real data! A few quick review of yesterday:

:::{.callout-note icon=false title="Bayesian statistical paradigm"}
1. A model for the data generating mechanism is specified in terms of probability distributions with unknown parameters. The model should be specified in a way that the questions you have of the data may be asked of the model parameters.

2. Your prior belief about the unknown parameters is expressed in a probabilistic way via the **prior distribution**, $p(\theta)$.

3. Data are collected and are modeled via a probability distribution, $p(\boldsymbol{y} | \theta)$ (i.e. the likelihood or sampling model)

4. Your updated belief in $\theta$ is expressed via the **posterior distribution**, according to Bayes' rule:
$$
p(\theta | \boldsymbol{y}) = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{p(\boldsymbol{y})} = \frac{p(\boldsymbol{y}|\theta)p(\theta)}{\int_\theta p(\boldsymbol{y}|\theta) p(\theta) d\theta}
$$

5. All subsequent inference is based on the posterior distribution. 
:::

:::{.callout-note icon=false title="Bayesian data analysis"}
1. Generate samples from the posterior distribution (using math, PPLs, etc)

2. Assess model convergence using trace plots and the Gelman-Rubin diagnostic.

3. Obtain posterior summaries of the model (means, medians, credibility intervals)

4. Assess model fit using posterior predictive checks

5. (Optionally) Compute metrics for model comparison (Bayes' factors, information criterion, LOO-CV)
:::



\newpage

\begin{landscape}
\thispagestyle{mylandscape}

\definecolor{lg}{RGB}{245,246,246}
\begin{tblr}{
colspec=lccccc, hline{2},
row{2,3,6,7,11,12,15,16,19,20,23,24} = {bg=lg},
cell{2}{2}={r=2}{c}, cell{2}{4}={r=2}{c}, cell{2}{5}={r=2}{c}, cell{2}{6}={r=2}{c},
cell{4}{2}={r=2}{c}, cell{4}{4}={r=2}{c}, cell{4}{5}={r=2}{c}, cell{4}{6}={r=2}{c},
cell{6}{2}={r=2}{c}, cell{6}{4}={r=2}{c}, cell{6}{5}={r=2}{c}, cell{6}{6}={r=2}{c},
cell{8}{1}={r=3}{l}, cell{8}{4}={r=3}{c}, cell{8}{5}={r=3}{c}, cell{8}{6}={r=3}{c},
cell{11}{2}={r=2}{c}, cell{11}{4}={r=2}{c}, cell{11}{5}={r=2}{c}, cell{11}{6}={r=2}{c},
cell{13}{4}={r=2}{c}, cell{13}{5}={r=2}{c}, cell{13}{6}={r=2}{c},
cell{15}{2}={r=2}{c}, cell{15}{4}={r=2}{c}, cell{15}{5}={r=2}{c}, cell{15}{6}={r=2}{c},
cell{17}{2}={r=2}{c}, cell{17}{4}={r=2}{c}, cell{17}{5}={r=2}{c}, cell{17}{6}={r=2}{c},
cell{19}{2}={r=2}{c}, cell{19}{4}={r=2}{c}, cell{19}{5}={r=2}{c}, cell{19}{6}={r=2}{c},
cell{21}{2}={r=2}{c}, cell{21}{4}={r=2}{c}, cell{21}{5}={r=2}{c}, cell{21}{6}={r=2}{c},
cell{23}{2}={r=2}{c}, cell{23}{4}={r=2}{c}, cell{23}{5}={r=2}{c}, cell{23}{6}={r=2}{c},
}

\textbf{Distribution} & \textbf{Parameters} & \textbf{Probability function} & \textbf{Mean} & \textbf{Variance} & \textbf{MGF} \\

Bernoulli & $p \in [0, 1]$ & $f(x) = p^x(1-p)^{1-x}$; & $p$ & $p(1-p)$ & $pe^t + (1 - p)$ \\
$\text{Bern}(p)$ & 3-2 & $x \in \{0, 1\}$ & 3-4 & 3-5 & 3-6 \\

Binomial & $p \in [0, 1]$ & $f(x) = \binom{n}{x}p^x(1-p)^{n-x}$; & $np$ & $np(1-p)$ & $\left[pe^t + (1 - p) \right]^n$ \\
$\text{Bin}(p)$ & 5-2 & $x \in \{0, 1, ..., n\}$ & 5-4 & 5-5 & 5-6 \\

Geometric & $p \in [0, 1]$ & $f(x) = p(1 - p)^{x-1}$; & $\frac{1}{p}$ & $\frac{1 - p}{p^2}$ & $\frac{pe^t}{1 - (1 - p)e^t}$\\
$\text{Geom}(p)$ & 7-2 & $x \in \{1, 2, \dots \}$ & 7-4 & 7-5 & 7-6 \\

Hypergeometric & $ N \in \{0, 1, \dots\}$ & $f(x) = \frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}}$; & $\frac{nr}{N}$ & $n \left( \frac{n}{r} \right) \left( \frac{N-r}{N} \right) \left( \frac{N-n}{N-1} \right)$ & Don't bother \\
9-1 & $r \in \{0, 1, \dots, N\}$ & $x \in \{0, 1, \dots, n\}$ if $n\leq r$, & 9-4 & 9-5 & 9-6 \\
10-1 & $n \in \{0, 1, \dots, N\}$ & $x \in \{0, 1, \dots, r\}$ if $n > r$ & 10-4 & 10-5 & 10-6 \\

Poisson & $\lambda > 0$ & $f(x) = \frac{e^{-\lambda}\lambda^x}{x!}$; & $\lambda$ & $\lambda$ & $\exp\left[\lambda(e^t - 1) \right]$ \\
$\text{Pois}(\lambda)$ & & $x \in \{0, 1, \dots\}$ & & & \\

Negative binomial & $r \in \{0, 1, \dots\}$ & $f(x) = \binom{x + r + 1}{x}p^r(1-p)^x$; & $\frac{r(1-p)}{p}$ & $\frac{r(1-p)}{p^2}$ & $\left(\frac{p}{1 - (1-p)e^t} \right)^r$ \\
$\text{NegBin}(r, p)$ & $p \in [0, 1]$ & $x \in \{0, 1, \dots\}$ & & & \\

Beta & $a, b > 0$ & $f(x) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1}$; & $\frac{a}{a+b}$ & $\frac{ab}{(a + b)^2(a + b + 1)}$ & DNE \\
$\text{beta(a, b)}$ & & $x \in (0, 1)$ & & & \\

Chi-square & $\nu \in \{1, 2, \dots\}$ & $f(x) = \frac{1}{2^{\nu/2}\Gamma(\frac{\nu}{2})}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}$; & $\nu$ & $2\nu$ & $(1-2t)^{-v/2}$ \\
$\chi^2_\nu$ & & $x > 0$ & & & \\

Exponential & $\lambda > 0$ & $f(x) = \lambda e^{-\lambda x}$; & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ & $\frac{\lambda}{\lambda - t}$ \\
$\text{Exp}(\lambda)$ & & $x > 0$ & & & \\

Gamma & $\alpha, \beta > 0$ & $f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1} e^{-\beta x}$; & $\frac{\alpha}{\beta}$ & $\frac{\alpha}{\beta^2}$ & $\left(1 - \frac{t}{\beta}\right)^{-\alpha}$ \\
$\text{Ga}(\alpha, \beta)$ & & $x > 0$ & & & \\

Normal & $\mu \in (-\infty, \infty)$ & $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2 \right\}$;  & $\mu$ & $\sigma^2$ & $\exp\left\{\mu t + \frac{t^2\sigma^2}{2}\right\}$ \\
$N(\mu, \sigma^2)$ & & $x \in (-\infty, \infty)$ & & & \\

Uniform & $\{\theta_1, \theta_2: \theta_1 < \theta_2\}$ & $f(x) = \frac{1}{\theta_2 - \theta_1}$;  & $\frac{1}{2}(\theta_1+\theta_2)$ & $\frac{1}{12}(\theta_2-\theta_1)^2$ & $\frac{e^{t\theta_2}-e^{t\theta_1}}{t(\theta_2 - \theta_1)}$ \\
$\text{Unif}(\theta_1, \theta_2)$ & & $\theta_1 \leq x \leq \theta_2$ & & & \\
\end{tblr}
\end{landscape}

\newpage

## Warm-up: Real-data example

Now let's practice on some real data! We will use a subset of the data Anna is working with. 

```{r}
`%notin%` <- Negate("%in%")
weeds <- readRDS("bcc_dat.rds") %>% 
  dplyr::select(Number_Tre:first3, Year_) %>%
  filter(PCT_ZoneTr %notin% c("-", "1165")) %>% # two rows
  mutate(PCT_ZoneTr = as.numeric(PCT_ZoneTr))
weeds
```

There are a number of potential questions to investigate for these data, including:

1. `Number_Tre ~ PCT_ZoneTr`

2. `Number_Tre ~ PCT_ZoneTr * Code`

3. `Number_Tre ~ PCT_ZoneTr * Code + first3`

For each, write out the model including the priors, and fit the model using either `nimble` or `stan`. Provide posterior summaries and interpret the results. 

### Nimble

```{r, eval = F}
library(nimble)
# nimble code for model
code <- nimbleCode({
  # priors 
  alpha ~ dnorm(0, sd = 10)
  beta ~ dnorm(0, sd = 10)
  
  # likelihood as a loop
  for(i in 1:n){
    log(lambda[i]) <- alpha + beta*PCT_ZoneTr[i]
    y[i] ~ dpois(lambda[i])
  }
})

# build model
rmodel <- nimbleModel(
  code = code, # model code
  data = list(y = weeds$Number_Tre), # list of data (the random stuff)
  constants = list(
    n = nrow(weeds), 
    PCT_ZoneTr = weeds$PCT_ZoneTr
  )
) # model in R
cmodel <- compileNimble(rmodel) # model in C++

# build MCMC
mcmc_conf <- configureMCMC(cmodel)
rmcmc <- buildMCMC(mcmc_conf) # MCMC in R
cmcmc <- compileNimble(rmcmc, project = cmodel) # MCMC in C++

# run the MCMC
samples <- runMCMC(
  cmcmc,
  niter = 10000, # number of posterior samples
  nburnin = 5000, # how many posterior samples to discard
  thin = 5, # interval of samples to keep 
  nchains = 4, # number of independent Markov chains to run 
  samplesAsCodaMCMC = TRUE # nice formatting
)
```

```{r, eval = F}
# convergence
as_tibble(do.call("rbind", samples)) %>% 
  mutate(
    iter = rep(1:nrow(samples[[1]]), length(samples)),
    chain = rep(1:length(samples), each = nrow(samples[[1]])) %>% factor(), 
  ) %>%
  dplyr::select(iter, chain, everything()) %>%
  pivot_longer(-c(1:2), names_to = "param", values_to = "trace") %>%
  arrange(param, chain, iter) %>%
  ggplot() + 
  geom_line(aes(x = iter, y = trace, col = chain)) +
  facet_wrap(~ param, scales = "free_y", nrow = 2) +
  theme_bw()

# summaries
library(posterior)
samples_posterior <- as_draws_matrix(samples)
posterior::summarise_draws(
  samples_posterior
)
bayestestR::hdi(samples) 
```

```{r, eval = F}
# posterior predictive checks 
library(bayesplot)
samples_mat <- do.call("rbind", samples)
ppc <- matrix(NA, 100, nrow(weeds)) # only do what were gonna plot
ndx <- sample(1:nrow(samples_mat), 100)
for(i in 1:100){
  lambda <- exp(samples_mat[ndx[i], 1] + samples_mat[ndx[i], 2] * weeds$PCT_ZoneTr)
  ppc[i,] <- rpois(ncol(ppc), lambda)
}

# can do it by hand to make it look a little nicer
ppc_df <- t(rbind(weeds$Number_Tre, ppc)) %>%
  as_tibble() %>%
  pivot_longer(everything(), names_to = "var", values_to = "val") %>%
  mutate(obs = ifelse(var == "V1", "y", "yrep")) 
ggplot() + 
  geom_density(
    data = ppc_df %>% filter(obs != "y"),
    aes(x = val, group = var), col = "lightblue", alpha = .5
  ) +  
  geom_density(
    data = ppc_df %>% filter(obs == "y"),
    aes(x = val), size = 1.1
  ) + 
  theme_bw() +
  labs(x = "")
```

```{r, eval = F}
# some model comparison metrics
library(loo)

## approximate leave one out cross validation
## need to get the log likelihood matrix first
### kind of slow...
llik_mat <- matrix(NA, nrow = nrow(samples_mat), ncol = nrow(weeds))
for(i in 1:nrow(samples_mat)){
  alpha <- samples_mat[i,1]
  beta <- samples_mat[i,2]
  llik_mat[i,] <- dpois(weeds$Number_Tre, exp(alpha + beta * weeds$PCT_ZoneTr), log = T)
}

loo1 <- loo(llik_mat)
loo1

## too many high k values, use WAIC
waic1 <- loo::waic(llik_mat)
waic1
```

### Stan


## Hierarchical modeling

```{r}
library(rstanarm)
data(radon)

radon %>%
  ggplot() + 
  geom_jitter(aes(x = log_radon, y = county)) +
  theme_bw()

radon %>%
  ggplot() + 
  geom_boxplot(aes(x = log_radon, y = county)) +
  theme_bw()
```





